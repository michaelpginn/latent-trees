{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T15:15:48.828166Z",
     "start_time": "2023-12-12T15:15:42.040687Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/2400 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ec60cc56635c47f68f945c3b5d4c7248"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/800 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4c2c09d2518947ecbb29b9f2a9e60647"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/800 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "793033e8dc9b47f594e7f0e173b11a77"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datasets\n",
    "from transformers import BertTokenizer, BertConfig, BertForSequenceClassification, TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "from src.eval import eval_preds\n",
    "from src.TreeTransformer import TreeBertForSequenceClassification\n",
    "\n",
    "dataset = datasets.load_dataset(\"michaelginn/latent-trees-agreement-ID\")\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "max_length = 100\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example['text'], max_length=max_length, truncation=True)\n",
    "dataset = dataset.map(tokenize_function, batched=True, load_from_cache_file=False)\n",
    "\n",
    "toy_dataset = dataset['train'].select(range(1, 11))\n",
    "\n",
    "id2label = {0: \"VIOLATION\", 1: \"GRAMMATICAL\"}\n",
    "label2id = {\"VIOLATION\": 0, \"GRAMMATICAL\": 1}\n",
    "\n",
    "pretrained = False\n",
    "if pretrained:\n",
    "    config = BertConfig.from_pretrained('bert-base-uncased', num_labels=2, id2label=id2label, label2id=label2id)\n",
    "else:\n",
    "    # Create random initialized BERT model\n",
    "    config = BertConfig(num_labels=2, id2label=id2label, label2id=label2id)\n",
    "\n",
    "model = TreeBertForSequenceClassification(config=config).to('mps')\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=f\"../training-checkpoints\",\n",
    "    learning_rate=2e-5,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=1,\n",
    "    save_strategy=\"epoch\",\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=10,\n",
    "    load_best_model_at_end=False,\n",
    "    logging_strategy='epoch',\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    labels = eval_pred.label_ids\n",
    "    preds = np.argmax(eval_pred.predictions, axis=-1)\n",
    "    print(eval_pred.predictions)\n",
    "    return eval_preds(preds, labels)\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=toy_dataset,\n",
    "    eval_dataset=toy_dataset, # dataset['test'].select(range(20)),\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# trainer.train()\n",
    "\n",
    "# preds = trainer.predict(dataset['test'].select(range(20)))\n",
    "# preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2023-12-13T01:24:31.092279Z",
     "start_time": "2023-12-13T01:24:26.067511Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[3.1623e-05, 7.0538e-01, 3.1623e-05, 3.1623e-05],\n",
      "         [7.0538e-01, 3.1623e-05, 7.0883e-01, 3.1623e-05],\n",
      "         [3.1623e-05, 7.0883e-01, 3.1623e-05, 3.1623e-05],\n",
      "         [3.1623e-05, 3.1623e-05, 3.1623e-05, 3.1623e-05]]],\n",
      "       grad_fn=<AddBackward0>), tensor([[[6.3245e-05, 9.1367e-01, 5.0001e-01, 4.7435e-05],\n",
      "         [9.1367e-01, 6.3245e-05, 9.1476e-01, 5.4039e-05],\n",
      "         [5.0001e-01, 9.1476e-01, 6.3245e-05, 6.3247e-05],\n",
      "         [4.7435e-05, 5.4039e-05, 6.3247e-05, 6.3245e-05]]],\n",
      "       grad_fn=<AddBackward0>), tensor([[[9.4865e-05, 9.7482e-01, 8.3579e-01, 8.4483e-05],\n",
      "         [9.7482e-01, 9.4865e-05, 9.7493e-01, 8.9478e-05],\n",
      "         [8.3579e-01, 9.7493e-01, 9.4865e-05, 9.4869e-05],\n",
      "         [8.4483e-05, 8.9478e-05, 9.4869e-05, 9.4865e-05]]],\n",
      "       grad_fn=<AddBackward0>), tensor([[[1.2649e-04, 9.9262e-01, 9.5038e-01, 1.2178e-04],\n",
      "         [9.9262e-01, 1.2649e-04, 9.9267e-01, 1.2411e-04],\n",
      "         [9.5038e-01, 9.9267e-01, 1.2649e-04, 1.2649e-04],\n",
      "         [1.2178e-04, 1.2411e-04, 1.2649e-04, 1.2649e-04]]],\n",
      "       grad_fn=<AddBackward0>), tensor([[[1.5810e-04, 9.9784e-01, 9.8534e-01, 1.5626e-04],\n",
      "         [9.9784e-01, 1.5810e-04, 9.9785e-01, 1.5718e-04],\n",
      "         [9.8534e-01, 9.9785e-01, 1.5810e-04, 1.5811e-04],\n",
      "         [1.5626e-04, 1.5718e-04, 1.5811e-04, 1.5810e-04]]],\n",
      "       grad_fn=<AddBackward0>), tensor([[[1.8972e-04, 9.9936e-01, 9.9569e-01, 1.8905e-04],\n",
      "         [9.9936e-01, 1.8972e-04, 9.9937e-01, 1.8939e-04],\n",
      "         [9.9569e-01, 9.9937e-01, 1.8972e-04, 1.8973e-04],\n",
      "         [1.8905e-04, 1.8939e-04, 1.8973e-04, 1.8972e-04]]],\n",
      "       grad_fn=<AddBackward0>), tensor([[[2.2134e-04, 9.9981e-01, 9.9874e-01, 2.2111e-04],\n",
      "         [9.9981e-01, 2.2134e-04, 9.9982e-01, 2.2123e-04],\n",
      "         [9.9874e-01, 9.9982e-01, 2.2134e-04, 2.2135e-04],\n",
      "         [2.2111e-04, 2.2123e-04, 2.2135e-04, 2.2134e-04]]],\n",
      "       grad_fn=<AddBackward0>), tensor([[[2.5295e-04, 9.9995e-01, 9.9963e-01, 2.5289e-04],\n",
      "         [9.9995e-01, 2.5295e-04, 9.9995e-01, 2.5293e-04],\n",
      "         [9.9963e-01, 9.9995e-01, 2.5295e-04, 2.5297e-04],\n",
      "         [2.5289e-04, 2.5293e-04, 2.5297e-04, 2.5295e-04]]],\n",
      "       grad_fn=<AddBackward0>), tensor([[[2.8457e-04, 9.9998e-01, 9.9989e-01, 2.8456e-04],\n",
      "         [9.9998e-01, 2.8457e-04, 9.9998e-01, 2.8457e-04],\n",
      "         [9.9989e-01, 9.9998e-01, 2.8457e-04, 2.8459e-04],\n",
      "         [2.8456e-04, 2.8457e-04, 2.8459e-04, 2.8457e-04]]],\n",
      "       grad_fn=<AddBackward0>), tensor([[[3.1618e-04, 1.0000e+00, 9.9997e-01, 3.1619e-04],\n",
      "         [1.0000e+00, 3.1618e-04, 1.0000e+00, 3.1620e-04],\n",
      "         [9.9997e-01, 1.0000e+00, 3.1618e-04, 3.1620e-04],\n",
      "         [3.1619e-04, 3.1620e-04, 3.1620e-04, 3.1618e-04]]],\n",
      "       grad_fn=<AddBackward0>), tensor([[[3.4780e-04, 1.0000e+00, 9.9999e-01, 3.4781e-04],\n",
      "         [1.0000e+00, 3.4780e-04, 1.0000e+00, 3.4781e-04],\n",
      "         [9.9999e-01, 1.0000e+00, 3.4780e-04, 3.4782e-04],\n",
      "         [3.4781e-04, 3.4781e-04, 3.4782e-04, 3.4780e-04]]],\n",
      "       grad_fn=<AddBackward0>), tensor([[[3.7941e-04, 1.0000e+00, 1.0000e+00, 3.7943e-04],\n",
      "         [1.0000e+00, 3.7941e-04, 1.0000e+00, 3.7943e-04],\n",
      "         [1.0000e+00, 1.0000e+00, 3.7941e-04, 3.7943e-04],\n",
      "         [3.7943e-04, 3.7943e-04, 3.7943e-04, 3.7941e-04]]],\n",
      "       grad_fn=<AddBackward0>))\n"
     ]
    },
    {
     "data": {
      "text/plain": "SequenceClassifierOutputWithConstituentAttention(loss=None, logits=tensor([[-0.0768,  0.0854]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertConfig\n",
    "from src.TreeTransformer import TreeBertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "\n",
    "config = BertConfig(num_labels=2)\n",
    "model = TreeBertForSequenceClassification(config=config)\n",
    "model(input_ids=torch.tensor([[1, 2, 3, 0]]), attention_mask=torch.tensor([[1, 1, 1, 0]]), return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

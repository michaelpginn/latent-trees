{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T02:43:39.209526Z",
     "start_time": "2023-12-13T02:43:27.509239Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/2400 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "23aab6897e3c4fa6845823f62f871fda"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/800 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "264d93cd04b4430aa971a000cf5e53db"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/800 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7007423267154cf9808548a5b9e89632"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mmichael-ginn\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "wandb version 0.16.1 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.16.0"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>/Users/milesper/Documents/Research/LatentTrees/latent-trees/wandb/run-20231212_194337-lanpia51</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/michael-ginn/huggingface/runs/lanpia51' target=\"_blank\">genial-wildflower-19</a></strong> to <a href='https://wandb.ai/michael-ginn/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/michael-ginn/huggingface' target=\"_blank\">https://wandb.ai/michael-ginn/huggingface</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/michael-ginn/huggingface/runs/lanpia51' target=\"_blank\">https://wandb.ai/michael-ginn/huggingface/runs/lanpia51</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 13, 768])\n",
      "torch.Size([2, 13])\n",
      "torch.Size([13, 13])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (13) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 61\u001B[0m\n\u001B[1;32m     49\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m eval_preds(preds, labels)\n\u001B[1;32m     52\u001B[0m trainer \u001B[38;5;241m=\u001B[39m Trainer(\n\u001B[1;32m     53\u001B[0m     model,\n\u001B[1;32m     54\u001B[0m     args,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     58\u001B[0m     tokenizer\u001B[38;5;241m=\u001B[39mtokenizer\n\u001B[1;32m     59\u001B[0m )\n\u001B[0;32m---> 61\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     63\u001B[0m \u001B[38;5;66;03m# preds = trainer.predict(dataset['test'].select(range(20)))\u001B[39;00m\n\u001B[1;32m     64\u001B[0m \u001B[38;5;66;03m# preds\u001B[39;00m\n",
      "File \u001B[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/transformers/trainer.py:1555\u001B[0m, in \u001B[0;36mTrainer.train\u001B[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[0m\n\u001B[1;32m   1553\u001B[0m         hf_hub_utils\u001B[38;5;241m.\u001B[39menable_progress_bars()\n\u001B[1;32m   1554\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1555\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner_training_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1556\u001B[0m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1557\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1558\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrial\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1559\u001B[0m \u001B[43m        \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1560\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/transformers/trainer.py:1860\u001B[0m, in \u001B[0;36mTrainer._inner_training_loop\u001B[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[0m\n\u001B[1;32m   1857\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallback_handler\u001B[38;5;241m.\u001B[39mon_step_begin(args, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol)\n\u001B[1;32m   1859\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccelerator\u001B[38;5;241m.\u001B[39maccumulate(model):\n\u001B[0;32m-> 1860\u001B[0m     tr_loss_step \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1862\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m   1863\u001B[0m     args\u001B[38;5;241m.\u001B[39mlogging_nan_inf_filter\n\u001B[1;32m   1864\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torch_tpu_available()\n\u001B[1;32m   1865\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (torch\u001B[38;5;241m.\u001B[39misnan(tr_loss_step) \u001B[38;5;129;01mor\u001B[39;00m torch\u001B[38;5;241m.\u001B[39misinf(tr_loss_step))\n\u001B[1;32m   1866\u001B[0m ):\n\u001B[1;32m   1867\u001B[0m     \u001B[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001B[39;00m\n\u001B[1;32m   1868\u001B[0m     tr_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m tr_loss \u001B[38;5;241m/\u001B[39m (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mglobal_step \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_globalstep_last_logged)\n",
      "File \u001B[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/transformers/trainer.py:2725\u001B[0m, in \u001B[0;36mTrainer.training_step\u001B[0;34m(self, model, inputs)\u001B[0m\n\u001B[1;32m   2722\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m loss_mb\u001B[38;5;241m.\u001B[39mreduce_mean()\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m   2724\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompute_loss_context_manager():\n\u001B[0;32m-> 2725\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompute_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2727\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mn_gpu \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m   2728\u001B[0m     loss \u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mmean()  \u001B[38;5;66;03m# mean() to average on multi-gpu parallel training\u001B[39;00m\n",
      "File \u001B[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/transformers/trainer.py:2748\u001B[0m, in \u001B[0;36mTrainer.compute_loss\u001B[0;34m(self, model, inputs, return_outputs)\u001B[0m\n\u001B[1;32m   2746\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   2747\u001B[0m     labels \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 2748\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2749\u001B[0m \u001B[38;5;66;03m# Save past state if it exists\u001B[39;00m\n\u001B[1;32m   2750\u001B[0m \u001B[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001B[39;00m\n\u001B[1;32m   2751\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mpast_index \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Documents/Research/LatentTrees/latent-trees/src/TreeTransformer.py:545\u001B[0m, in \u001B[0;36mTreeBertForSequenceClassification.forward\u001B[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    537\u001B[0m \u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    538\u001B[0m \u001B[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001B[39;00m\n\u001B[1;32m    539\u001B[0m \u001B[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001B[39;00m\n\u001B[1;32m    540\u001B[0m \u001B[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001B[39;00m\n\u001B[1;32m    541\u001B[0m \u001B[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001B[39;00m\n\u001B[1;32m    542\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    543\u001B[0m return_dict \u001B[38;5;241m=\u001B[39m return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39muse_return_dict\n\u001B[0;32m--> 545\u001B[0m outputs: BaseModelOutputWithPoolingAndCrossAttentionsAndConstituentAttention \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbert\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    546\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    547\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    548\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtoken_type_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken_type_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    549\u001B[0m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    550\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    551\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    552\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    553\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    554\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    555\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    557\u001B[0m pooled_output \u001B[38;5;241m=\u001B[39m outputs[\u001B[38;5;241m1\u001B[39m]\n\u001B[1;32m    559\u001B[0m pooled_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout(pooled_output)\n",
      "File \u001B[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Documents/Research/LatentTrees/latent-trees/src/TreeTransformer.py:475\u001B[0m, in \u001B[0;36mTreeBertModel.forward\u001B[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    466\u001B[0m head_mask \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_head_mask(head_mask, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mnum_hidden_layers)\n\u001B[1;32m    468\u001B[0m embedding_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membeddings(\n\u001B[1;32m    469\u001B[0m     input_ids\u001B[38;5;241m=\u001B[39minput_ids,\n\u001B[1;32m    470\u001B[0m     position_ids\u001B[38;5;241m=\u001B[39mposition_ids,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    473\u001B[0m     past_key_values_length\u001B[38;5;241m=\u001B[39mpast_key_values_length,\n\u001B[1;32m    474\u001B[0m )\n\u001B[0;32m--> 475\u001B[0m encoder_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    476\u001B[0m \u001B[43m    \u001B[49m\u001B[43membedding_output\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    477\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    478\u001B[0m \u001B[43m    \u001B[49m\u001B[43mextended_attention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextended_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    479\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    480\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    481\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_extended_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    482\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    483\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    484\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    485\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    486\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    487\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    488\u001B[0m sequence_output \u001B[38;5;241m=\u001B[39m encoder_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    489\u001B[0m pooled_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpooler(sequence_output) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpooler \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Documents/Research/LatentTrees/latent-trees/src/TreeTransformer.py:299\u001B[0m, in \u001B[0;36mTreeBertEncoder.forward\u001B[0;34m(self, hidden_states, attention_mask, extended_attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    288\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mcheckpoint\u001B[38;5;241m.\u001B[39mcheckpoint(\n\u001B[1;32m    289\u001B[0m         create_custom_forward(layer_module),\n\u001B[1;32m    290\u001B[0m         hidden_states,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    296\u001B[0m         encoder_attention_mask,\n\u001B[1;32m    297\u001B[0m     )\n\u001B[1;32m    298\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 299\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[43mlayer_module\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    300\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    301\u001B[0m \u001B[43m        \u001B[49m\u001B[43mconstituent_prior\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    302\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    303\u001B[0m \u001B[43m        \u001B[49m\u001B[43mextended_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    304\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlayer_head_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    305\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    306\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    307\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    308\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    309\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    311\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m layer_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    312\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m use_cache:\n",
      "File \u001B[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Documents/Research/LatentTrees/latent-trees/src/TreeTransformer.py:202\u001B[0m, in \u001B[0;36mTreeBertLayer.forward\u001B[0;34m(self, hidden_states, constituent_prior, attention_mask, extended_attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001B[0m\n\u001B[1;32m    189\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\n\u001B[1;32m    190\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    191\u001B[0m     hidden_states: torch\u001B[38;5;241m.\u001B[39mTensor,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    200\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[torch\u001B[38;5;241m.\u001B[39mTensor]:\n\u001B[1;32m    201\u001B[0m     \u001B[38;5;66;03m# Compute constituency attention in order to mask attention\u001B[39;00m\n\u001B[0;32m--> 202\u001B[0m     constituent_attention_output, neighboring_attention \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroup_attention\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconstituent_prior\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    204\u001B[0m     \u001B[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001B[39;00m\n\u001B[1;32m    205\u001B[0m     self_attention_output: torch\u001B[38;5;241m.\u001B[39mTensor \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mattention(\n\u001B[1;32m    206\u001B[0m         query\u001B[38;5;241m=\u001B[39mhidden_states,\n\u001B[1;32m    207\u001B[0m         key\u001B[38;5;241m=\u001B[39mhidden_states,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    210\u001B[0m         mask\u001B[38;5;241m=\u001B[39mattention_mask,\n\u001B[1;32m    211\u001B[0m     )\n",
      "File \u001B[0;32m~/.pyenv/versions/3.10.0/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Documents/Research/LatentTrees/latent-trees/src/TreeTransformer.py:148\u001B[0m, in \u001B[0;36mGroupAttention.forward\u001B[0;34m(self, hidden_states, attention_mask, prior)\u001B[0m\n\u001B[1;32m    146\u001B[0m \u001B[38;5;28mprint\u001B[39m(attention_mask\u001B[38;5;241m.\u001B[39msize())\n\u001B[1;32m    147\u001B[0m \u001B[38;5;28mprint\u001B[39m((a\u001B[38;5;241m+\u001B[39mc)\u001B[38;5;241m.\u001B[39msize())\n\u001B[0;32m--> 148\u001B[0m mask \u001B[38;5;241m=\u001B[39m \u001B[43mattention_mask\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m&\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43ma\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mc\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    150\u001B[0m key \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlinear_key(hidden_states)\n\u001B[1;32m    151\u001B[0m query \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlinear_query(hidden_states)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: The size of tensor a (2) must match the size of tensor b (13) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from transformers import BertTokenizer, BertConfig, BertForSequenceClassification, TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "from src.eval import eval_preds\n",
    "from src.TreeTransformer import TreeBertForSequenceClassification\n",
    "\n",
    "dataset = datasets.load_dataset(\"michaelginn/latent-trees-agreement-ID\")\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "max_length = 100\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example['text'], max_length=max_length, truncation=True)\n",
    "dataset = dataset.map(tokenize_function, batched=True, load_from_cache_file=False)\n",
    "\n",
    "toy_dataset = dataset['train'].select(range(1, 11))\n",
    "\n",
    "id2label = {0: \"VIOLATION\", 1: \"GRAMMATICAL\"}\n",
    "label2id = {\"VIOLATION\": 0, \"GRAMMATICAL\": 1}\n",
    "\n",
    "pretrained = False\n",
    "if pretrained:\n",
    "    config = BertConfig.from_pretrained('bert-base-uncased', num_labels=2, id2label=id2label, label2id=label2id)\n",
    "else:\n",
    "    # Create random initialized BERT model\n",
    "    config = BertConfig(num_labels=2, id2label=id2label, label2id=label2id)\n",
    "\n",
    "model = TreeBertForSequenceClassification(config=config).to('mps')\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=f\"../training-checkpoints\",\n",
    "    learning_rate=2e-5,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=1,\n",
    "    save_strategy=\"epoch\",\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=10,\n",
    "    load_best_model_at_end=False,\n",
    "    logging_strategy='epoch',\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    labels = eval_pred.label_ids\n",
    "    preds = np.argmax(eval_pred.predictions, axis=-1)\n",
    "    print(eval_pred.predictions)\n",
    "    return eval_preds(preds, labels)\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['eval'], # dataset['test'].select(range(20)),\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# preds = trainer.predict(dataset['test'].select(range(20)))\n",
    "# preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2023-12-13T03:44:05.812826Z",
     "start_time": "2023-12-13T03:44:00.735953Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden torch.Size([2, 4, 768])\n",
      "a tensor([[0, 1, 0, 0],\n",
      "        [0, 0, 1, 0],\n",
      "        [0, 0, 0, 1],\n",
      "        [0, 0, 0, 0]], dtype=torch.int32)\n",
      "b tensor([[1, 0, 0, 0],\n",
      "        [0, 1, 0, 0],\n",
      "        [0, 0, 1, 0],\n",
      "        [0, 0, 0, 1]], dtype=torch.int32)\n",
      "c tensor([[0, 0, 0, 0],\n",
      "        [1, 0, 0, 0],\n",
      "        [0, 1, 0, 0],\n",
      "        [0, 0, 1, 0]], dtype=torch.int32)\n",
      "attention mask torch.Size([2, 4])\n",
      "a+c torch.Size([4, 4])\n",
      "mask torch.Size([2, 4, 4])\n",
      "scores torch.Size([2, 4, 4])\n",
      "scores masked torch.Size([2, 4, 4])\n",
      "query torch.Size([2, 12, 4, 64])\n",
      "combined tensor([[[1, 1, 1, 0],\n",
      "         [1, 1, 1, 0],\n",
      "         [1, 1, 1, 0],\n",
      "         [1, 1, 1, 1]],\n",
      "\n",
      "        [[1, 1, 0, 0],\n",
      "         [1, 1, 0, 0],\n",
      "         [1, 1, 1, 0],\n",
      "         [1, 1, 0, 1]]])\n",
      "combined unsqueezed tensor([[[[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]]]])\n",
      "scores torch.Size([2, 12, 4, 4])\n",
      "hidden torch.Size([2, 4, 768])\n",
      "a tensor([[0, 1, 0, 0],\n",
      "        [0, 0, 1, 0],\n",
      "        [0, 0, 0, 1],\n",
      "        [0, 0, 0, 0]], dtype=torch.int32)\n",
      "b tensor([[1, 0, 0, 0],\n",
      "        [0, 1, 0, 0],\n",
      "        [0, 0, 1, 0],\n",
      "        [0, 0, 0, 1]], dtype=torch.int32)\n",
      "c tensor([[0, 0, 0, 0],\n",
      "        [1, 0, 0, 0],\n",
      "        [0, 1, 0, 0],\n",
      "        [0, 0, 1, 0]], dtype=torch.int32)\n",
      "attention mask torch.Size([2, 4])\n",
      "a+c torch.Size([4, 4])\n",
      "mask torch.Size([2, 4, 4])\n",
      "scores torch.Size([2, 4, 4])\n",
      "scores masked torch.Size([2, 4, 4])\n",
      "query torch.Size([2, 12, 4, 64])\n",
      "combined tensor([[[1, 1, 1, 0],\n",
      "         [1, 1, 1, 0],\n",
      "         [1, 1, 1, 0],\n",
      "         [1, 1, 1, 1]],\n",
      "\n",
      "        [[1, 1, 0, 0],\n",
      "         [1, 1, 0, 0],\n",
      "         [1, 1, 1, 0],\n",
      "         [1, 1, 0, 1]]])\n",
      "combined unsqueezed tensor([[[[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]]]])\n",
      "scores torch.Size([2, 12, 4, 4])\n",
      "hidden torch.Size([2, 4, 768])\n",
      "a tensor([[0, 1, 0, 0],\n",
      "        [0, 0, 1, 0],\n",
      "        [0, 0, 0, 1],\n",
      "        [0, 0, 0, 0]], dtype=torch.int32)\n",
      "b tensor([[1, 0, 0, 0],\n",
      "        [0, 1, 0, 0],\n",
      "        [0, 0, 1, 0],\n",
      "        [0, 0, 0, 1]], dtype=torch.int32)\n",
      "c tensor([[0, 0, 0, 0],\n",
      "        [1, 0, 0, 0],\n",
      "        [0, 1, 0, 0],\n",
      "        [0, 0, 1, 0]], dtype=torch.int32)\n",
      "attention mask torch.Size([2, 4])\n",
      "a+c torch.Size([4, 4])\n",
      "mask torch.Size([2, 4, 4])\n",
      "scores torch.Size([2, 4, 4])\n",
      "scores masked torch.Size([2, 4, 4])\n",
      "query torch.Size([2, 12, 4, 64])\n",
      "combined tensor([[[1, 1, 1, 0],\n",
      "         [1, 1, 1, 0],\n",
      "         [1, 1, 1, 0],\n",
      "         [1, 1, 1, 1]],\n",
      "\n",
      "        [[1, 1, 0, 0],\n",
      "         [1, 1, 0, 0],\n",
      "         [1, 1, 1, 0],\n",
      "         [1, 1, 0, 1]]])\n",
      "combined unsqueezed tensor([[[[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]]]])\n",
      "scores torch.Size([2, 12, 4, 4])\n",
      "hidden torch.Size([2, 4, 768])\n",
      "a tensor([[0, 1, 0, 0],\n",
      "        [0, 0, 1, 0],\n",
      "        [0, 0, 0, 1],\n",
      "        [0, 0, 0, 0]], dtype=torch.int32)\n",
      "b tensor([[1, 0, 0, 0],\n",
      "        [0, 1, 0, 0],\n",
      "        [0, 0, 1, 0],\n",
      "        [0, 0, 0, 1]], dtype=torch.int32)\n",
      "c tensor([[0, 0, 0, 0],\n",
      "        [1, 0, 0, 0],\n",
      "        [0, 1, 0, 0],\n",
      "        [0, 0, 1, 0]], dtype=torch.int32)\n",
      "attention mask torch.Size([2, 4])\n",
      "a+c torch.Size([4, 4])\n",
      "mask torch.Size([2, 4, 4])\n",
      "scores torch.Size([2, 4, 4])\n",
      "scores masked torch.Size([2, 4, 4])\n",
      "query torch.Size([2, 12, 4, 64])\n",
      "combined tensor([[[1, 1, 1, 0],\n",
      "         [1, 1, 1, 0],\n",
      "         [1, 1, 1, 0],\n",
      "         [1, 1, 1, 1]],\n",
      "\n",
      "        [[1, 1, 0, 0],\n",
      "         [1, 1, 0, 0],\n",
      "         [1, 1, 1, 0],\n",
      "         [1, 1, 0, 1]]])\n",
      "combined unsqueezed tensor([[[[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]]]])\n",
      "scores torch.Size([2, 12, 4, 4])\n",
      "hidden torch.Size([2, 4, 768])\n",
      "a tensor([[0, 1, 0, 0],\n",
      "        [0, 0, 1, 0],\n",
      "        [0, 0, 0, 1],\n",
      "        [0, 0, 0, 0]], dtype=torch.int32)\n",
      "b tensor([[1, 0, 0, 0],\n",
      "        [0, 1, 0, 0],\n",
      "        [0, 0, 1, 0],\n",
      "        [0, 0, 0, 1]], dtype=torch.int32)\n",
      "c tensor([[0, 0, 0, 0],\n",
      "        [1, 0, 0, 0],\n",
      "        [0, 1, 0, 0],\n",
      "        [0, 0, 1, 0]], dtype=torch.int32)\n",
      "attention mask torch.Size([2, 4])\n",
      "a+c torch.Size([4, 4])\n",
      "mask torch.Size([2, 4, 4])\n",
      "scores torch.Size([2, 4, 4])\n",
      "scores masked torch.Size([2, 4, 4])\n",
      "query torch.Size([2, 12, 4, 64])\n",
      "combined tensor([[[1, 1, 1, 0],\n",
      "         [1, 1, 1, 0],\n",
      "         [1, 1, 1, 0],\n",
      "         [1, 1, 1, 1]],\n",
      "\n",
      "        [[1, 1, 0, 0],\n",
      "         [1, 1, 0, 0],\n",
      "         [1, 1, 1, 0],\n",
      "         [1, 1, 0, 1]]])\n",
      "combined unsqueezed tensor([[[[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]]]])\n",
      "scores torch.Size([2, 12, 4, 4])\n",
      "hidden torch.Size([2, 4, 768])\n",
      "a tensor([[0, 1, 0, 0],\n",
      "        [0, 0, 1, 0],\n",
      "        [0, 0, 0, 1],\n",
      "        [0, 0, 0, 0]], dtype=torch.int32)\n",
      "b tensor([[1, 0, 0, 0],\n",
      "        [0, 1, 0, 0],\n",
      "        [0, 0, 1, 0],\n",
      "        [0, 0, 0, 1]], dtype=torch.int32)\n",
      "c tensor([[0, 0, 0, 0],\n",
      "        [1, 0, 0, 0],\n",
      "        [0, 1, 0, 0],\n",
      "        [0, 0, 1, 0]], dtype=torch.int32)\n",
      "attention mask torch.Size([2, 4])\n",
      "a+c torch.Size([4, 4])\n",
      "mask torch.Size([2, 4, 4])\n",
      "scores torch.Size([2, 4, 4])\n",
      "scores masked torch.Size([2, 4, 4])\n",
      "query torch.Size([2, 12, 4, 64])\n",
      "combined tensor([[[1, 1, 1, 0],\n",
      "         [1, 1, 1, 0],\n",
      "         [1, 1, 1, 0],\n",
      "         [1, 1, 1, 1]],\n",
      "\n",
      "        [[1, 1, 0, 0],\n",
      "         [1, 1, 0, 0],\n",
      "         [1, 1, 1, 0],\n",
      "         [1, 1, 0, 1]]])\n",
      "combined unsqueezed tensor([[[[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]]]])\n",
      "scores torch.Size([2, 12, 4, 4])\n",
      "hidden torch.Size([2, 4, 768])\n",
      "a tensor([[0, 1, 0, 0],\n",
      "        [0, 0, 1, 0],\n",
      "        [0, 0, 0, 1],\n",
      "        [0, 0, 0, 0]], dtype=torch.int32)\n",
      "b tensor([[1, 0, 0, 0],\n",
      "        [0, 1, 0, 0],\n",
      "        [0, 0, 1, 0],\n",
      "        [0, 0, 0, 1]], dtype=torch.int32)\n",
      "c tensor([[0, 0, 0, 0],\n",
      "        [1, 0, 0, 0],\n",
      "        [0, 1, 0, 0],\n",
      "        [0, 0, 1, 0]], dtype=torch.int32)\n",
      "attention mask torch.Size([2, 4])\n",
      "a+c torch.Size([4, 4])\n",
      "mask torch.Size([2, 4, 4])\n",
      "scores torch.Size([2, 4, 4])\n",
      "scores masked torch.Size([2, 4, 4])\n",
      "query torch.Size([2, 12, 4, 64])\n",
      "combined tensor([[[1, 1, 1, 0],\n",
      "         [1, 1, 1, 0],\n",
      "         [1, 1, 1, 0],\n",
      "         [1, 1, 1, 1]],\n",
      "\n",
      "        [[1, 1, 0, 0],\n",
      "         [1, 1, 0, 0],\n",
      "         [1, 1, 1, 0],\n",
      "         [1, 1, 0, 1]]])\n",
      "combined unsqueezed tensor([[[[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]]]])\n",
      "scores torch.Size([2, 12, 4, 4])\n",
      "hidden torch.Size([2, 4, 768])\n",
      "a tensor([[0, 1, 0, 0],\n",
      "        [0, 0, 1, 0],\n",
      "        [0, 0, 0, 1],\n",
      "        [0, 0, 0, 0]], dtype=torch.int32)\n",
      "b tensor([[1, 0, 0, 0],\n",
      "        [0, 1, 0, 0],\n",
      "        [0, 0, 1, 0],\n",
      "        [0, 0, 0, 1]], dtype=torch.int32)\n",
      "c tensor([[0, 0, 0, 0],\n",
      "        [1, 0, 0, 0],\n",
      "        [0, 1, 0, 0],\n",
      "        [0, 0, 1, 0]], dtype=torch.int32)\n",
      "attention mask torch.Size([2, 4])\n",
      "a+c torch.Size([4, 4])\n",
      "mask torch.Size([2, 4, 4])\n",
      "scores torch.Size([2, 4, 4])\n",
      "scores masked torch.Size([2, 4, 4])\n",
      "query torch.Size([2, 12, 4, 64])\n",
      "combined tensor([[[1, 1, 1, 0],\n",
      "         [1, 1, 1, 0],\n",
      "         [1, 1, 1, 0],\n",
      "         [1, 1, 1, 1]],\n",
      "\n",
      "        [[1, 1, 0, 0],\n",
      "         [1, 1, 0, 0],\n",
      "         [1, 1, 1, 0],\n",
      "         [1, 1, 0, 1]]])\n",
      "combined unsqueezed tensor([[[[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]]]])\n",
      "scores torch.Size([2, 12, 4, 4])\n",
      "hidden torch.Size([2, 4, 768])\n",
      "a tensor([[0, 1, 0, 0],\n",
      "        [0, 0, 1, 0],\n",
      "        [0, 0, 0, 1],\n",
      "        [0, 0, 0, 0]], dtype=torch.int32)\n",
      "b tensor([[1, 0, 0, 0],\n",
      "        [0, 1, 0, 0],\n",
      "        [0, 0, 1, 0],\n",
      "        [0, 0, 0, 1]], dtype=torch.int32)\n",
      "c tensor([[0, 0, 0, 0],\n",
      "        [1, 0, 0, 0],\n",
      "        [0, 1, 0, 0],\n",
      "        [0, 0, 1, 0]], dtype=torch.int32)\n",
      "attention mask torch.Size([2, 4])\n",
      "a+c torch.Size([4, 4])\n",
      "mask torch.Size([2, 4, 4])\n",
      "scores torch.Size([2, 4, 4])\n",
      "scores masked torch.Size([2, 4, 4])\n",
      "query torch.Size([2, 12, 4, 64])\n",
      "combined tensor([[[1, 1, 1, 0],\n",
      "         [1, 1, 1, 0],\n",
      "         [1, 1, 1, 0],\n",
      "         [1, 1, 1, 1]],\n",
      "\n",
      "        [[1, 1, 0, 0],\n",
      "         [1, 1, 0, 0],\n",
      "         [1, 1, 1, 0],\n",
      "         [1, 1, 0, 1]]])\n",
      "combined unsqueezed tensor([[[[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]]]])\n",
      "scores torch.Size([2, 12, 4, 4])\n",
      "hidden torch.Size([2, 4, 768])\n",
      "a tensor([[0, 1, 0, 0],\n",
      "        [0, 0, 1, 0],\n",
      "        [0, 0, 0, 1],\n",
      "        [0, 0, 0, 0]], dtype=torch.int32)\n",
      "b tensor([[1, 0, 0, 0],\n",
      "        [0, 1, 0, 0],\n",
      "        [0, 0, 1, 0],\n",
      "        [0, 0, 0, 1]], dtype=torch.int32)\n",
      "c tensor([[0, 0, 0, 0],\n",
      "        [1, 0, 0, 0],\n",
      "        [0, 1, 0, 0],\n",
      "        [0, 0, 1, 0]], dtype=torch.int32)\n",
      "attention mask torch.Size([2, 4])\n",
      "a+c torch.Size([4, 4])\n",
      "mask torch.Size([2, 4, 4])\n",
      "scores torch.Size([2, 4, 4])\n",
      "scores masked torch.Size([2, 4, 4])\n",
      "query torch.Size([2, 12, 4, 64])\n",
      "combined tensor([[[1, 1, 1, 0],\n",
      "         [1, 1, 1, 0],\n",
      "         [1, 1, 1, 0],\n",
      "         [1, 1, 1, 1]],\n",
      "\n",
      "        [[1, 1, 0, 0],\n",
      "         [1, 1, 0, 0],\n",
      "         [1, 1, 1, 0],\n",
      "         [1, 1, 0, 1]]])\n",
      "combined unsqueezed tensor([[[[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]]]])\n",
      "scores torch.Size([2, 12, 4, 4])\n",
      "hidden torch.Size([2, 4, 768])\n",
      "a tensor([[0, 1, 0, 0],\n",
      "        [0, 0, 1, 0],\n",
      "        [0, 0, 0, 1],\n",
      "        [0, 0, 0, 0]], dtype=torch.int32)\n",
      "b tensor([[1, 0, 0, 0],\n",
      "        [0, 1, 0, 0],\n",
      "        [0, 0, 1, 0],\n",
      "        [0, 0, 0, 1]], dtype=torch.int32)\n",
      "c tensor([[0, 0, 0, 0],\n",
      "        [1, 0, 0, 0],\n",
      "        [0, 1, 0, 0],\n",
      "        [0, 0, 1, 0]], dtype=torch.int32)\n",
      "attention mask torch.Size([2, 4])\n",
      "a+c torch.Size([4, 4])\n",
      "mask torch.Size([2, 4, 4])\n",
      "scores torch.Size([2, 4, 4])\n",
      "scores masked torch.Size([2, 4, 4])\n",
      "query torch.Size([2, 12, 4, 64])\n",
      "combined tensor([[[1, 1, 1, 0],\n",
      "         [1, 1, 1, 0],\n",
      "         [1, 1, 1, 0],\n",
      "         [1, 1, 1, 1]],\n",
      "\n",
      "        [[1, 1, 0, 0],\n",
      "         [1, 1, 0, 0],\n",
      "         [1, 1, 1, 0],\n",
      "         [1, 1, 0, 1]]])\n",
      "combined unsqueezed tensor([[[[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]]]])\n",
      "scores torch.Size([2, 12, 4, 4])\n",
      "hidden torch.Size([2, 4, 768])\n",
      "a tensor([[0, 1, 0, 0],\n",
      "        [0, 0, 1, 0],\n",
      "        [0, 0, 0, 1],\n",
      "        [0, 0, 0, 0]], dtype=torch.int32)\n",
      "b tensor([[1, 0, 0, 0],\n",
      "        [0, 1, 0, 0],\n",
      "        [0, 0, 1, 0],\n",
      "        [0, 0, 0, 1]], dtype=torch.int32)\n",
      "c tensor([[0, 0, 0, 0],\n",
      "        [1, 0, 0, 0],\n",
      "        [0, 1, 0, 0],\n",
      "        [0, 0, 1, 0]], dtype=torch.int32)\n",
      "attention mask torch.Size([2, 4])\n",
      "a+c torch.Size([4, 4])\n",
      "mask torch.Size([2, 4, 4])\n",
      "scores torch.Size([2, 4, 4])\n",
      "scores masked torch.Size([2, 4, 4])\n",
      "query torch.Size([2, 12, 4, 64])\n",
      "combined tensor([[[1, 1, 1, 0],\n",
      "         [1, 1, 1, 0],\n",
      "         [1, 1, 1, 0],\n",
      "         [1, 1, 1, 1]],\n",
      "\n",
      "        [[1, 1, 0, 0],\n",
      "         [1, 1, 0, 0],\n",
      "         [1, 1, 1, 0],\n",
      "         [1, 1, 0, 1]]])\n",
      "combined unsqueezed tensor([[[[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]],\n",
      "\n",
      "         [[1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]],\n",
      "\n",
      "         [[1, 1, 0, 0],\n",
      "          [1, 1, 0, 0],\n",
      "          [1, 1, 1, 0],\n",
      "          [1, 1, 0, 1]]]])\n",
      "scores torch.Size([2, 12, 4, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": "SequenceClassifierOutputWithConstituentAttention(loss=None, logits=tensor([[-0.0112, -0.4408],\n        [-0.0646, -0.1347]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertConfig\n",
    "from src.TreeTransformer import TreeBertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "\n",
    "config = BertConfig(num_labels=2)\n",
    "model = TreeBertForSequenceClassification(config=config)\n",
    "model(input_ids=torch.tensor([[1, 2, 3, 0], [2, 1, 0, 1]]), attention_mask=torch.tensor([[1, 1, 1, 0], [1, 1, 0, 0]]), return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T03:23:28.332198Z",
     "start_time": "2023-12-13T03:23:28.327185Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 3, 4])"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.IntTensor([[1, 1, 1, 0]])\n",
    "mask = torch.IntTensor([[0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]])\n",
    "\n",
    "results = []\n",
    "for row in a:\n",
    "    results.append(row & mask)\n",
    "result = torch.stack(results)\n",
    "result.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0, 1, 0, 0],\n        [0, 0, 1, 0],\n        [0, 0, 0, 1],\n        [0, 0, 0, 0]], dtype=torch.int32)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = torch.from_numpy(np.diag(np.ones(4 - 1, dtype=np.int32), 1))\n",
    "a"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T03:08:51.656401Z",
     "start_time": "2023-12-13T03:08:51.653437Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

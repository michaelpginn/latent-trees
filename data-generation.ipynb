{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93b0202f-1d8c-4289-9579-d181347ccc9c",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-11-25T21:54:45.724942Z",
     "start_time": "2023-11-25T21:54:45.001617Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "\"'man' [0.3333333333333333] | 'woman' [0.3333333333333333] | 'girl' [0.3333333333333333]\""
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import PCFG, Nonterminal\n",
    "from nltk.parse.generate import generate\n",
    "\n",
    "def equal_production(terminals, total=1):\n",
    "    \"\"\"Shorthand to write a list of terminals that are all equally likely\"\"\"\n",
    "    terminals = terminals.split(' | ')\n",
    "    return ' | '.join([f\"'{terminal}' [{total/len(terminals)}]\" for terminal in terminals])\n",
    "\n",
    "equal_production('man | woman | girl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c62c8e58-e298-42b0-b811-1293931f6ac6",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-11-25T22:02:03.546463Z",
     "start_time": "2023-11-25T22:02:03.444630Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Randomly generate sentences using CFG\n",
    "def weighted_choice(choices):\n",
    "    total = sum(w for c, w in choices)\n",
    "    r = random.uniform(0, total)\n",
    "    upto = 0\n",
    "    for c, w in choices:\n",
    "        if upto + w >= r:\n",
    "            return c\n",
    "        upto += w\n",
    "\n",
    "def generate_sentence(grammar, symbol=Nonterminal('S')):\n",
    "    productions = grammar.productions(lhs=symbol)\n",
    "    chosen_prod = weighted_choice([(prod, prod.prob()) for prod in productions])\n",
    "    \n",
    "    sentence = []\n",
    "    # print(symbol)\n",
    "    for sym in chosen_prod.rhs():\n",
    "        if isinstance(sym, Nonterminal):\n",
    "            sentence.extend(generate_sentence(grammar, sym))\n",
    "        else:\n",
    "            sentence.append(sym)\n",
    "            \n",
    "    return sentence\n",
    "\n",
    "# Morphology\n",
    "from pyfoma import *\n",
    "\n",
    "fsts = {}\n",
    "fsts['lex'] = FST.re(\"[a-zA-Z\\+]*\")\n",
    "\n",
    "fsts['sib']       = FST.re(\"s|sh|z|zh|ch|x\")\n",
    "fsts['C']         = FST.re(\"[a-z] - [aeiou]\")\n",
    "fsts['sibrk']     = FST.re(\"$^rewrite('':e / $sib _ \\+ s)\", fsts)\n",
    "fsts['yrule']     = FST.re(\"$^rewrite(y:(ie) / $C _ \\+ s)\", fsts)\n",
    "fsts['cleanup']   = FST.re(\"$^rewrite(\\+:'')\")\n",
    "fsts['grammar']   = FST.re(\"$lex @ $sibrk @ $yrule @ $cleanup\", fsts)\n",
    "\n",
    "def fix_morphology(words):\n",
    "    \"\"\"Combines morpheme clusters into proper words using an FST\"\"\"\n",
    "    combined_words = []\n",
    "    for word in words:\n",
    "        if word[0] == \"+\":\n",
    "            combined_words[-1] += word\n",
    "        else:\n",
    "            combined_words.append(word)\n",
    "    return [list(fsts['grammar'].generate(word))[0] for word in combined_words]\n",
    "\n",
    "def sample_sentences(grammar, n):\n",
    "    sents = [generate_sentence(grammar) for _ in tqdm(range(n))]\n",
    "    sents = [' '.join(fix_morphology(sent)) for sent in sents]\n",
    "    # sents = [sent.capitalize() + \".\" for sent in sents]\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3602393f-fd6b-4c30-828b-ef997fc8e5a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T22:02:11.499492Z",
     "start_time": "2023-11-25T22:02:11.474731Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 23039.30it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "['those wugs love a boy',\n 'the bird thinks',\n 'a wug walks',\n 'they love a dude',\n 'a girl hugs them',\n 'these wugs kick us',\n 'the happy cheese laughs',\n 'she kicks the girl',\n 'a lady that ponders loves a cat',\n 'they run',\n 'a rabbit fights these sparkling dogs',\n 'a cat ponders',\n 'he fights me',\n 'the cat laughs',\n 'the wug ponders',\n 'she loves me',\n 'the sparkling cats that run kiss these rabbits',\n 'a cheese that kisses these mad girls runs',\n 'I walk and laugh',\n 'we ponder']"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pcfg = PCFG.fromstring(f\"\"\"\n",
    "S             -> NP_3Sg_nom VP_3Sg [0.5] | NP_nom VP [0.5]\n",
    "\n",
    "VP_3Sg        -> VT '+s' NP_acc [0.475] | VI '+s' [0.475] | VP_3Sg 'and' VP_3Sg [0.05]\n",
    "VP            -> VT      NP_acc [0.475] | VI      [0.475] | VP     'and' VP     [0.05]\n",
    "\n",
    "NP_3Sg_nom    -> 'he' [0.25] | 'she' [0.25] | NP_common_Sg [0.5]\n",
    "NP_common_Sg  -> Det_Sg N_bar_common_Sg [1]\n",
    "Det_Sg        -> {equal_production('the | a')}\n",
    "\n",
    "NP_nom        -> {equal_production('I | you | we | they', total=0.5)} | NP_common_Pl [0.5]\n",
    "NP_common_Pl  -> Det_Pl N_bar_common_Pl [0.8] | NP_common_Pl 'and' NP_common_Pl [0.2]\n",
    "Det_Pl        -> {equal_production('the | those | these')}\n",
    "\n",
    "NP_acc        -> {equal_production('me | you | us | them', total=0.30)} | NP_common_Pl [0.35] | NP_common_Sg [0.35]\n",
    "\n",
    "N_bar_common_Sg  -> Adj N_bar_common_Sg [0.2] | N_common Rel_Sg [0.2] | N_common [0.6]\n",
    "N_bar_common_Pl  -> Adj N_bar_common_Pl [0.2] | N_common '+s' Rel_Pl [0.15] | N_common '+s' [0.65]\n",
    "N_common      -> {equal_production('girl | boy | cat | turtle | rutabaga | duck | cheese | dude | rabbit | wug | linguist | physicist | lady | dog | cat | bird')}\n",
    "\n",
    "Rel_Sg         -> 'that' VP_3Sg [1]\n",
    "Rel_Pl         -> 'that' VP [1]\n",
    "\n",
    "VI            -> {equal_production('run | walk | think | laugh | ponder')}\n",
    "VT            -> {equal_production('kick | kiss | hug | punch | fight | love')}\n",
    "\n",
    "Adj           -> {equal_production('big | small | happy | mad | red | blue | sparkling | shiny')}\n",
    "\"\"\")\n",
    "\n",
    "sample_sentences(pcfg, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "308f4216-02cf-4071-8a3f-d12b9b982a94",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-11-25T22:17:30.253702Z",
     "start_time": "2023-11-25T22:17:30.249811Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'the cat that runs ponder and hug the duck'"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# agreement_violations = PCFG.fromstring(f\"\"\"\n",
    "# S             -> NP_3Sg_nom VP_3Sg [0.5] | NP_nom VP [0.5]\n",
    "#\n",
    "# VP_3Sg        -> VT      NP_acc [0.475] | VI      [0.475] | VP_3Sg 'and' VP_3Sg [0.05]\n",
    "# VP            -> VT '+s' NP_acc [0.475] | VI '+s' [0.475] | VP     'and' VP     [0.05]\n",
    "#\n",
    "# NP_3Sg_nom    -> 'he' [0.25] | 'she' [0.25] | NP_common_Sg [0.5]\n",
    "# NP_common_Sg  -> Det_Sg N_bar_common_Sg [1]\n",
    "# Det_Sg        -> {equal_production('the | a')}\n",
    "#\n",
    "# NP_nom        -> {equal_production('I | you | we | they', total=0.5)} | NP_common_Pl [0.5]\n",
    "# NP_common_Pl  -> Det_Pl N_bar_common_Pl [0.8] | NP_common_Pl 'and' NP_common_Pl [0.2]\n",
    "# Det_Pl        -> {equal_production('the | those | these')}\n",
    "#\n",
    "# NP_acc        -> {equal_production('me | you | us | them', total=0.30)} | NP_common_Pl [0.35] | NP_common_Sg [0.35]\n",
    "#\n",
    "# N_bar_common_Sg  -> Adj N_bar_common_Sg [0.2] | N_common Rel_Sg [0.15] | N_common [0.65]\n",
    "# N_bar_common_Pl  -> Adj N_bar_common_Pl [0.2] | N_common '+s' Rel_Pl [0.15] | N_common '+s' [0.65]\n",
    "# N_common      -> {equal_production('girl | boy | cat | turtle | rutabaga | duck | cheese | dude | rabbit | wug | linguist | physicist | lady | dog | cat | bird')}\n",
    "#\n",
    "# Rel_Sg         -> 'that' VP_3Sg [1]\n",
    "# Rel_Pl         -> 'that' VP [1]\n",
    "#\n",
    "# VI            -> {equal_production('run | walk | think | laugh | ponder')}\n",
    "# VT            -> {equal_production('kick | kiss | hug | punch | fight | love')}\n",
    "#\n",
    "# Adj           -> {equal_production('big | small | happy | mad | red | blue | sparkling | shiny')}\n",
    "# \"\"\")\n",
    "#\n",
    "# sample_sentences(agreement_violations, 20)\n",
    "\n",
    "verb_sing = [\"runs\", \"walks\", \"thinks\", \"laughs\", \"ponders\", \"kicks\", \"kisses\", \"hugs\", \"punches\", \"fights\", \"loves\"]\n",
    "verb_pl = [\"run\", \"walk\", \"think\", \"laugh\", \"ponder\", \"kick\", \"kiss\", \"hug\", \"punch\", \"fight\", \"love\"]\n",
    "\n",
    "def deform_sentence(sentence: str):\n",
    "    \"\"\"Deforms a sentence by randomly switching one or more verbs from singular to plural or vice versa\"\"\"\n",
    "    words = sentence.split(' ')\n",
    "    verb_indices = []\n",
    "    for i, word in enumerate(words):\n",
    "        if word in verb_sing or word in verb_pl:\n",
    "            verb_indices.append(i)\n",
    "\n",
    "    indices_to_deform = random.sample(verb_indices, random.randint(1, len(verb_indices)))\n",
    "    for index in indices_to_deform:\n",
    "        word = words[index]\n",
    "        if word in verb_sing:\n",
    "            verb_index = verb_sing.index(word)\n",
    "            words[index] = verb_pl[verb_index]\n",
    "        elif word in verb_pl:\n",
    "            verb_index = verb_pl.index(word)\n",
    "            words[index] = verb_sing[verb_index]\n",
    "\n",
    "    return ' '.join(words)\n",
    "\n",
    "deform_sentence('the cat that runs ponders and hugs the duck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b120a2a8-57d4-44a1-9ac0-3f0fcc12c9de",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-11-26T02:52:06.814235Z",
     "start_time": "2023-11-26T02:51:52.886120Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:00<00:00, 39714.84it/s]\n",
      "100%|██████████| 2000/2000 [00:00<00:00, 54913.28it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "Stringifying the column:   0%|          | 0/4000 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e54ce77a46d44fc98677cbdd9debef10"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Casting to class labels:   0%|          | 0/4000 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6f0571dbb5af4b17a32dfa1f999baa06"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9920067801a9496da16808e858b6a884"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Creating parquet from Arrow format:   0%|          | 0/3 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a1db12dbb0c74aa7965d1940f96cab58"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Deleting unused files from dataset repository:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "187fba44d88843ab803e0915b7deeb3b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1f9c8c2aef1545eb831a8813071743c5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "07e4d2fe3f544a659acf275fb1c1ce16"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Deleting unused files from dataset repository:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f0dc345699e74daca8be7523768a8e91"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "132b9c079c1a45209d72ab44e98fadbf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dc6dccaced6b4c54ba46a7262d63c65e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Deleting unused files from dataset repository:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e2605737b88d448791dd472e4759084b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading metadata:   0%|          | 0.00/749 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "79ce6cccabed49bd942d599c8e954461"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "random.seed(1)\n",
    "valid_num = 2000\n",
    "invalid_num = 2000\n",
    "valid = sample_sentences(pcfg, valid_num)\n",
    "invalid = [deform_sentence(sentence) for sentence in sample_sentences(pcfg, invalid_num)]\n",
    "\n",
    "dataset = datasets.Dataset.from_dict({\"text\": valid + invalid, \"labels\": [1] * valid_num + [0] * invalid_num}).shuffle()\n",
    "\n",
    "dataset = dataset.class_encode_column('labels')\n",
    "dataset = dataset.train_test_split(test_size=0.4, stratify_by_column='labels')\n",
    "dataset_eval_test = dataset['test'].train_test_split(test_size=0.5, stratify_by_column='labels')\n",
    "dataset = datasets.DatasetDict({\n",
    "    'train': dataset['train'],\n",
    "    'eval': dataset_eval_test['train'],\n",
    "    'test': dataset_eval_test['test']\n",
    "})\n",
    "\n",
    "dataset.push_to_hub(\"michaelginn/latent-trees-agreement-ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "noun_sing = [\"girl\", \"boy\", \"cat\", \"turtle\", \"rutabaga\", \"duck\", \"cheese\", \"dude\", \"rabbit\", \"wug\", \"linguist\", \"physicist\", \"lady\", \"dog\", \"cat\", \"bird\", \"he\", \"she\"]\n",
    "noun_pl = [\"girls\", \"boys\", \"cats\", \"turtles\", \"rutabagas\", \"ducks\", \"cheeses\", \"dudes\", \"rabbits\", \"wugs\", \"linguists\", \"physicists\", \"ladies\", \"dogs\", \"cats\", \"birds\", \"I\", \"you\",  \"me\", \"us\", \"them\", \"they\", \"we\"]\n",
    "\n",
    "verb_sing = [\"runs\", \"walks\", \"thinks\", \"laughs\", \"ponders\", \"kicks\", \"kisses\", \"hugs\", \"punches\", \"fights\", \"loves\"]\n",
    "verb_pl = [\"run\", \"walk\", \"think\", \"laugh\", \"ponder\", \"kick\", \"kiss\", \"hug\", \"punch\", \"fight\", \"love\"]\n",
    "\n",
    "def check_linear_heuristic(sentence: str):\n",
    "    # Returns false if the sentence fails the linear heuristic (verb should agree with most recent noun)\n",
    "    most_recent_noun = None\n",
    "    most_recent_noun_num = None # 'pl' or 'sg'\n",
    "\n",
    "    for word in sentence.split(' '):\n",
    "        if word in noun_sing:\n",
    "            most_recent_noun = word\n",
    "            most_recent_noun_num = 'sg'\n",
    "        elif word in noun_pl:\n",
    "            most_recent_noun = word\n",
    "            most_recent_noun_num = 'pl'\n",
    "        elif word in verb_sing:\n",
    "            if most_recent_noun_num == 'pl':\n",
    "                return False\n",
    "        elif word in verb_pl:\n",
    "            if most_recent_noun_num == 'sg':\n",
    "                return False\n",
    "    return True\n",
    "\n",
    "print(check_linear_heuristic('the linguist that punches these rutabagas thinks'))\n",
    "print(check_linear_heuristic('the linguist that punches this rutabaga thinks'))\n",
    "print(check_linear_heuristic('the linguist that punches these rutabagas think'))\n",
    "print(check_linear_heuristic('they thinks'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-26T02:52:13.998596Z",
     "start_time": "2023-11-26T02:52:13.996383Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:00<00:00, 51620.11it/s]\n",
      "100%|██████████| 200000/200000 [00:03<00:00, 53286.97it/s]\n",
      "100%|██████████| 20000/20000 [00:00<00:00, 379442.82it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 544110.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passes both heuristics: 19116\n",
      "Fails both heuristics: 198239\n",
      "Passes hierarch, fails linear: 884\n",
      "Fails hierarch, passes linear: 1761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "random.seed(1)\n",
    "\n",
    "\n",
    "valid_num = 20000\n",
    "invalid_num = 200000\n",
    "valid = sample_sentences(pcfg, valid_num)\n",
    "invalid = [deform_sentence(sentence) for sentence in sample_sentences(pcfg, invalid_num)]\n",
    "\n",
    "# Filter sents that pass/fail true heuristic to also pass/fail linear heuristic so we have an ambiguous training dataset\n",
    "both_valid = []\n",
    "true_valid_but_linear_invalid = []\n",
    "for valid_sent in tqdm(valid):\n",
    "    if check_linear_heuristic(valid_sent):\n",
    "        both_valid.append(valid_sent)\n",
    "    else:\n",
    "        true_valid_but_linear_invalid.append(valid_sent)\n",
    "\n",
    "both_invalid = []\n",
    "true_invalid_but_linear_valid = []\n",
    "for invalid_sent in tqdm(invalid):\n",
    "    if not check_linear_heuristic(invalid_sent):\n",
    "        both_invalid.append(invalid_sent)\n",
    "    else:\n",
    "        true_invalid_but_linear_valid.append(invalid_sent)\n",
    "\n",
    "print(f\"Passes both heuristics: {len(both_valid)}\")\n",
    "print(f\"Fails both heuristics: {len(both_invalid)}\")\n",
    "print(f\"Passes hierarch, fails linear: {len(true_valid_but_linear_invalid)}\")\n",
    "print(f\"Fails hierarch, passes linear: {len(true_invalid_but_linear_valid)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-26T02:53:35.529899Z",
     "start_time": "2023-11-26T02:52:17.445730Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "data": {
      "text/plain": "Stringifying the column:   0%|          | 0/3200 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0a12e27e014f409793c5e2ce80ce37fe"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Casting to class labels:   0%|          | 0/3200 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f240117efb4f45d8b95998ffebe3b16b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Stringifying the column:   0%|          | 0/800 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "20f32d3da0ba4f29be1f7a80136939a1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Casting to class labels:   0%|          | 0/800 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "00e182020cc4447ca21baacffd8eda79"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "86581c9f61544845afc473bca2a93117"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Creating parquet from Arrow format:   0%|          | 0/3 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a53c725aeb424a059bfc51dab0eb13d3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Deleting unused files from dataset repository:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cbd0b6eb6f9240a7827883428a3110e6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a5816e98e1094d39ab5f9d9261003ab2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "52624e5b62714eb9804cfd713a26436c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Deleting unused files from dataset repository:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "73c965a761d34130be32cacf4a70c9b7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f6fbd4ca2181496c8e2a5faa6396a5b9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "758192b7a0d849ee88182a13b52a534f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Deleting unused files from dataset repository:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b2ce749b1bb94d259297f23b12de21cf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading metadata:   0%|          | 0.00/748 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cd7629fbcc2142d6a9bb5f15af6faecc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create generalization scenario\n",
    "train_examples = 3200\n",
    "\n",
    "dataset_gen_train = datasets.Dataset.from_dict({\"text\": both_valid[:1600] + both_invalid[:1600], \"labels\": [1] * 1600 + [0] * 1600}).shuffle()\n",
    "dataset_gen_eval = datasets.Dataset.from_dict({\"text\": true_valid_but_linear_invalid[:400] + true_invalid_but_linear_valid[:400], \"labels\": [1] * 400 + [0] * 400}).shuffle()\n",
    "\n",
    "dataset_gen_train_eval = dataset_gen_train.class_encode_column('labels').train_test_split(test_size=0.25, stratify_by_column='labels')\n",
    "dataset_gen_test = dataset_gen_eval.class_encode_column('labels')\n",
    "\n",
    "gen_dataset_dict = datasets.DatasetDict({\n",
    "    'train': dataset_gen_train_eval['train'],\n",
    "    'eval': dataset_gen_train_eval['test'],\n",
    "    'test': dataset_gen_test\n",
    "})\n",
    "# dataset.train_test_split(test_size=0.3, stratify_by_column='labels')\n",
    "gen_dataset_dict.push_to_hub(\"michaelginn/latent-trees-agreement-GEN\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-26T02:57:14.263896Z",
     "start_time": "2023-11-26T02:57:09.742401Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['text', 'labels'],\n        num_rows: 2400\n    })\n    eval: Dataset({\n        features: ['text', 'labels'],\n        num_rows: 800\n    })\n    test: Dataset({\n        features: ['text', 'labels'],\n        num_rows: 800\n    })\n})"
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_dataset_dict"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-26T02:57:16.459666Z",
     "start_time": "2023-11-26T02:57:16.455376Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['text', 'labels'],\n        num_rows: 2400\n    })\n    eval: Dataset({\n        features: ['text', 'labels'],\n        num_rows: 800\n    })\n    test: Dataset({\n        features: ['text', 'labels'],\n        num_rows: 800\n    })\n})"
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-26T02:54:50.660489Z",
     "start_time": "2023-11-26T02:54:50.641244Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

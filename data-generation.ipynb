{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93b0202f-1d8c-4289-9579-d181347ccc9c",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-12-14T02:20:34.833679Z",
     "start_time": "2023-12-14T02:20:33.778062Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "\"'man' [0.3333333333333333] | 'woman' [0.3333333333333333] | 'girl' [0.3333333333333333]\""
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import PCFG, Nonterminal\n",
    "from nltk.parse.generate import generate\n",
    "\n",
    "def equal_production(terminals, total=1):\n",
    "    \"\"\"Shorthand to write a list of terminals that are all equally likely\"\"\"\n",
    "    terminals = terminals.split(' | ')\n",
    "    return ' | '.join([f\"'{terminal}' [{total/len(terminals)}]\" for terminal in terminals])\n",
    "\n",
    "equal_production('man | woman | girl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c62c8e58-e298-42b0-b811-1293931f6ac6",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-12-14T03:32:41.911356Z",
     "start_time": "2023-12-14T03:32:41.731630Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Randomly generate sentences using CFG\n",
    "def weighted_choice(choices):\n",
    "    total = sum(w for c, w in choices)\n",
    "    r = random.uniform(0, total)\n",
    "    upto = 0\n",
    "    for c, w in choices:\n",
    "        if upto + w >= r:\n",
    "            return c\n",
    "        upto += w\n",
    "\n",
    "def generate_sentence(grammar, symbol=Nonterminal('S'), depth=0):\n",
    "    productions = grammar.productions(lhs=symbol)\n",
    "    chosen_prod = weighted_choice([(prod, prod.prob()) for prod in productions])\n",
    "    \n",
    "    sentence = []\n",
    "    max_depth = depth\n",
    "\n",
    "    # print(symbol)\n",
    "    for sym in chosen_prod.rhs():\n",
    "        if isinstance(sym, Nonterminal):\n",
    "            nonterminal_is_relative_clause = sym == Nonterminal('Rel_Sg') or sym == Nonterminal('Rel_Pl')\n",
    "            phrase, new_max_depth = generate_sentence(grammar, sym, depth + 1 if nonterminal_is_relative_clause else depth)\n",
    "            sentence.extend(phrase)\n",
    "            max_depth = max(max_depth, new_max_depth)\n",
    "        else:\n",
    "            sentence.append(sym)\n",
    "            \n",
    "    return sentence, max_depth\n",
    "\n",
    "# Morphology\n",
    "from pyfoma import *\n",
    "\n",
    "fsts = {}\n",
    "fsts['lex'] = FST.re(\"[a-zA-Z\\+]*\")\n",
    "\n",
    "fsts['sib']       = FST.re(\"s|sh|z|zh|ch|x\")\n",
    "fsts['C']         = FST.re(\"[a-z] - [aeiou]\")\n",
    "fsts['sibrk']     = FST.re(\"$^rewrite('':e / $sib _ \\+ s)\", fsts)\n",
    "fsts['yrule']     = FST.re(\"$^rewrite(y:(ie) / $C _ \\+ s)\", fsts)\n",
    "fsts['cleanup']   = FST.re(\"$^rewrite(\\+:'')\")\n",
    "fsts['grammar']   = FST.re(\"$lex @ $sibrk @ $yrule @ $cleanup\", fsts)\n",
    "\n",
    "def fix_morphology(words):\n",
    "    \"\"\"Combines morpheme clusters into proper words using an FST\"\"\"\n",
    "    combined_words = []\n",
    "    for word in words:\n",
    "        if word[0] == \"+\":\n",
    "            combined_words[-1] += word\n",
    "        else:\n",
    "            combined_words.append(word)\n",
    "    return [list(fsts['grammar'].generate(word))[0] for word in combined_words]\n",
    "\n",
    "def sample_sentences(grammar, n):\n",
    "    sents = [generate_sentence(grammar) for _ in tqdm(range(n))]\n",
    "    sents = [(' '.join(fix_morphology(sent)), max_depth) for (sent, max_depth) in sents]\n",
    "    # sents = [sent.capitalize() + \".\" for sent in sents]\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3602393f-fd6b-4c30-828b-ef997fc8e5a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-14T03:32:43.142975Z",
     "start_time": "2023-12-14T03:32:43.130342Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 25739.82it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "[('she kicks a red red cat', 0),\n ('you run', 0),\n ('a girl that kisses the duck fights the ducks', 1),\n ('he hugs these cats and these cats that think', 1),\n ('they laugh', 0),\n ('he punches me', 0),\n ('she ponders and thinks', 0),\n ('the girls that kick those boys run', 1),\n ('they punch me', 0),\n ('you run', 0),\n ('a physicist punches us', 0),\n ('a cheese that loves the dude and laughs runs', 1),\n ('they laugh and run', 0),\n ('the cats and the rutabagas walk', 0),\n ('he kisses the girls', 0),\n ('a linguist that ponders thinks', 1),\n ('a turtle fights a cat that fights them', 1),\n ('he walks', 0),\n ('you walk', 0),\n ('we love those physicists', 0)]"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pcfg = PCFG.fromstring(f\"\"\"\n",
    "S             -> NP_3Sg_nom VP_3Sg [0.5] | NP_nom VP [0.5]\n",
    "\n",
    "VP_3Sg        -> VT '+s' NP_acc [0.475] | VI '+s' [0.475] | VP_3Sg 'and' VP_3Sg [0.05]\n",
    "VP            -> VT      NP_acc [0.475] | VI      [0.475] | VP     'and' VP     [0.05]\n",
    "\n",
    "NP_3Sg_nom    -> 'he' [0.25] | 'she' [0.25] | NP_common_Sg [0.5]\n",
    "NP_common_Sg  -> Det_Sg N_bar_common_Sg [1]\n",
    "Det_Sg        -> {equal_production('the | a')}\n",
    "\n",
    "NP_nom        -> {equal_production('I | you | we | they', total=0.5)} | NP_common_Pl [0.5]\n",
    "NP_common_Pl  -> Det_Pl N_bar_common_Pl [0.8] | NP_common_Pl 'and' NP_common_Pl [0.2]\n",
    "Det_Pl        -> {equal_production('the | those | these')}\n",
    "\n",
    "NP_acc        -> {equal_production('me | you | us | them', total=0.30)} | NP_common_Pl [0.35] | NP_common_Sg [0.35]\n",
    "\n",
    "N_bar_common_Sg  -> Adj N_bar_common_Sg [0.2] | N_common Rel_Sg [0.2] | N_common [0.6]\n",
    "N_bar_common_Pl  -> Adj N_bar_common_Pl [0.2] | N_common '+s' Rel_Pl [0.15] | N_common '+s' [0.65]\n",
    "N_common      -> {equal_production('girl | boy | cat | turtle | rutabaga | duck | cheese | dude | rabbit | wug | linguist | physicist | lady | dog | cat | bird')}\n",
    "\n",
    "Rel_Sg         -> 'that' VP_3Sg [1]\n",
    "Rel_Pl         -> 'that' VP [1]\n",
    "\n",
    "VI            -> {equal_production('run | walk | think | laugh | ponder')}\n",
    "VT            -> {equal_production('kick | kiss | hug | punch | fight | love')}\n",
    "\n",
    "Adj           -> {equal_production('big | small | happy | mad | red | blue | sparkling | shiny')}\n",
    "\"\"\")\n",
    "\n",
    "sample_sentences(pcfg, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "308f4216-02cf-4071-8a3f-d12b9b982a94",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-12-14T03:32:51.792879Z",
     "start_time": "2023-12-14T03:32:51.777752Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'the cat that runs ponders and hug the duck'"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# agreement_violations = PCFG.fromstring(f\"\"\"\n",
    "# S             -> NP_3Sg_nom VP_3Sg [0.5] | NP_nom VP [0.5]\n",
    "#\n",
    "# VP_3Sg        -> VT      NP_acc [0.475] | VI      [0.475] | VP_3Sg 'and' VP_3Sg [0.05]\n",
    "# VP            -> VT '+s' NP_acc [0.475] | VI '+s' [0.475] | VP     'and' VP     [0.05]\n",
    "#\n",
    "# NP_3Sg_nom    -> 'he' [0.25] | 'she' [0.25] | NP_common_Sg [0.5]\n",
    "# NP_common_Sg  -> Det_Sg N_bar_common_Sg [1]\n",
    "# Det_Sg        -> {equal_production('the | a')}\n",
    "#\n",
    "# NP_nom        -> {equal_production('I | you | we | they', total=0.5)} | NP_common_Pl [0.5]\n",
    "# NP_common_Pl  -> Det_Pl N_bar_common_Pl [0.8] | NP_common_Pl 'and' NP_common_Pl [0.2]\n",
    "# Det_Pl        -> {equal_production('the | those | these')}\n",
    "#\n",
    "# NP_acc        -> {equal_production('me | you | us | them', total=0.30)} | NP_common_Pl [0.35] | NP_common_Sg [0.35]\n",
    "#\n",
    "# N_bar_common_Sg  -> Adj N_bar_common_Sg [0.2] | N_common Rel_Sg [0.15] | N_common [0.65]\n",
    "# N_bar_common_Pl  -> Adj N_bar_common_Pl [0.2] | N_common '+s' Rel_Pl [0.15] | N_common '+s' [0.65]\n",
    "# N_common      -> {equal_production('girl | boy | cat | turtle | rutabaga | duck | cheese | dude | rabbit | wug | linguist | physicist | lady | dog | cat | bird')}\n",
    "#\n",
    "# Rel_Sg         -> 'that' VP_3Sg [1]\n",
    "# Rel_Pl         -> 'that' VP [1]\n",
    "#\n",
    "# VI            -> {equal_production('run | walk | think | laugh | ponder')}\n",
    "# VT            -> {equal_production('kick | kiss | hug | punch | fight | love')}\n",
    "#\n",
    "# Adj           -> {equal_production('big | small | happy | mad | red | blue | sparkling | shiny')}\n",
    "# \"\"\")\n",
    "#\n",
    "# sample_sentences(agreement_violations, 20)\n",
    "\n",
    "verb_sing = [\"runs\", \"walks\", \"thinks\", \"laughs\", \"ponders\", \"kicks\", \"kisses\", \"hugs\", \"punches\", \"fights\", \"loves\"]\n",
    "verb_pl = [\"run\", \"walk\", \"think\", \"laugh\", \"ponder\", \"kick\", \"kiss\", \"hug\", \"punch\", \"fight\", \"love\"]\n",
    "\n",
    "def deform_sentence(sentence: str):\n",
    "    \"\"\"Deforms a sentence by randomly switching one or more verbs from singular to plural or vice versa\"\"\"\n",
    "    words = sentence.split(' ')\n",
    "    verb_indices = []\n",
    "    for i, word in enumerate(words):\n",
    "        if word in verb_sing or word in verb_pl:\n",
    "            verb_indices.append(i)\n",
    "\n",
    "    indices_to_deform = random.sample(verb_indices, random.randint(1, len(verb_indices)))\n",
    "    for index in indices_to_deform:\n",
    "        word = words[index]\n",
    "        if word in verb_sing:\n",
    "            verb_index = verb_sing.index(word)\n",
    "            words[index] = verb_pl[verb_index]\n",
    "        elif word in verb_pl:\n",
    "            verb_index = verb_pl.index(word)\n",
    "            words[index] = verb_sing[verb_index]\n",
    "\n",
    "    return ' '.join(words)\n",
    "\n",
    "deform_sentence('the cat that runs ponders and hugs the duck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b120a2a8-57d4-44a1-9ac0-3f0fcc12c9de",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-12-14T03:34:58.214956Z",
     "start_time": "2023-12-14T03:34:53.986653Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:00<00:00, 33469.42it/s]\n",
      "100%|██████████| 2000/2000 [00:00<00:00, 37011.61it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "Stringifying the column:   0%|          | 0/4000 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "04b178a384224391a4718d66c362f6e9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Casting to class labels:   0%|          | 0/4000 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "458e1192543843a3b836ed4431ebe4a8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e13e8c865a33450d9b466105adbd3a57"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Creating parquet from Arrow format:   0%|          | 0/3 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b15c90cddcfc482e84682a9f5b235109"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "64152b8fea55456d84c2d314c8364d17"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "38c25f3e489c4566895287e55f35a2c4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0a51fdab52e046daa9b81343956d86f4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "67046faa6cc94383aa910adc92a22575"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "README.md:   0%|          | 0.00/748 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "26fba686891042e2a5f33b1bc6e1c6b1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "random.seed(1)\n",
    "valid_num = 2000\n",
    "invalid_num = 2000\n",
    "valid = sample_sentences(pcfg, valid_num)\n",
    "invalid = [(deform_sentence(sentence), max_depth) for sentence, max_depth in sample_sentences(pcfg, invalid_num)]\n",
    "\n",
    "texts = [sent for sent, _ in (valid + invalid)]\n",
    "labels = [1] * valid_num + [0] * invalid_num\n",
    "depths = [depth for _, depth in (valid + invalid)]\n",
    "\n",
    "dataset = datasets.Dataset.from_dict({\"text\": texts, \"labels\": labels, \"depth\": depths}).shuffle()\n",
    "\n",
    "dataset = dataset.class_encode_column('labels')\n",
    "dataset = dataset.train_test_split(test_size=0.4, stratify_by_column='labels')\n",
    "dataset_eval_test = dataset['test'].train_test_split(test_size=0.5, stratify_by_column='labels')\n",
    "dataset = datasets.DatasetDict({\n",
    "    'train': dataset['train'],\n",
    "    'eval': dataset_eval_test['train'],\n",
    "    'test': dataset_eval_test['test']\n",
    "})\n",
    "\n",
    "dataset.push_to_hub(\"michaelginn/latent-trees-agreement-ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "noun_sing = [\"girl\", \"boy\", \"cat\", \"turtle\", \"rutabaga\", \"duck\", \"cheese\", \"dude\", \"rabbit\", \"wug\", \"linguist\", \"physicist\", \"lady\", \"dog\", \"cat\", \"bird\", \"he\", \"she\"]\n",
    "noun_pl = [\"girls\", \"boys\", \"cats\", \"turtles\", \"rutabagas\", \"ducks\", \"cheeses\", \"dudes\", \"rabbits\", \"wugs\", \"linguists\", \"physicists\", \"ladies\", \"dogs\", \"cats\", \"birds\", \"I\", \"you\",  \"me\", \"us\", \"them\", \"they\", \"we\"]\n",
    "\n",
    "verb_sing = [\"runs\", \"walks\", \"thinks\", \"laughs\", \"ponders\", \"kicks\", \"kisses\", \"hugs\", \"punches\", \"fights\", \"loves\"]\n",
    "verb_pl = [\"run\", \"walk\", \"think\", \"laugh\", \"ponder\", \"kick\", \"kiss\", \"hug\", \"punch\", \"fight\", \"love\"]\n",
    "\n",
    "def check_linear_heuristic(sentence: str):\n",
    "    # Returns false if the sentence fails the linear heuristic (verb should agree with most recent noun)\n",
    "    most_recent_noun = None\n",
    "    most_recent_noun_num = None # 'pl' or 'sg'\n",
    "\n",
    "    for word in sentence.split(' '):\n",
    "        if word in noun_sing:\n",
    "            most_recent_noun = word\n",
    "            most_recent_noun_num = 'sg'\n",
    "        elif word in noun_pl:\n",
    "            most_recent_noun = word\n",
    "            most_recent_noun_num = 'pl'\n",
    "        elif word in verb_sing:\n",
    "            if most_recent_noun_num == 'pl':\n",
    "                return False\n",
    "        elif word in verb_pl:\n",
    "            if most_recent_noun_num == 'sg':\n",
    "                return False\n",
    "    return True\n",
    "\n",
    "print(check_linear_heuristic('the linguist that punches these rutabagas thinks'))\n",
    "print(check_linear_heuristic('the linguist that punches this rutabaga thinks'))\n",
    "print(check_linear_heuristic('the linguist that punches these rutabagas think'))\n",
    "print(check_linear_heuristic('they thinks'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T03:35:34.182217Z",
     "start_time": "2023-12-14T03:35:34.166388Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "random.seed(1)\n",
    "\n",
    "def generate_with_heuristic(grammar, n_valid, n_invalid):\n",
    "    valid = sample_sentences(grammar, n_valid)\n",
    "    invalid = [(deform_sentence(sentence), depth) for sentence, depth in sample_sentences(grammar, n_invalid)]\n",
    "\n",
    "    # Filter sents that pass/fail true heuristic to also pass/fail linear heuristic so we have an ambiguous training dataset\n",
    "    both_valid = []\n",
    "    true_valid_but_linear_invalid = []\n",
    "    for valid_sent, depth in tqdm(valid):\n",
    "        if check_linear_heuristic(valid_sent):\n",
    "            both_valid.append((valid_sent, depth))\n",
    "        else:\n",
    "            true_valid_but_linear_invalid.append((valid_sent, depth))\n",
    "\n",
    "    both_invalid = []\n",
    "    true_invalid_but_linear_valid = []\n",
    "    for invalid_sent, depth in tqdm(invalid):\n",
    "        if not check_linear_heuristic(invalid_sent):\n",
    "            both_invalid.append((invalid_sent, depth))\n",
    "        else:\n",
    "            true_invalid_but_linear_valid.append((invalid_sent, depth))\n",
    "\n",
    "    print(f\"Passes both heuristics: {len(both_valid)}\")\n",
    "    print(f\"Fails both heuristics: {len(both_invalid)}\")\n",
    "    print(f\"Passes hierarch, fails linear: {len(true_valid_but_linear_invalid)}\")\n",
    "    print(f\"Fails hierarch, passes linear: {len(true_invalid_but_linear_valid)}\")\n",
    "\n",
    "    return both_valid, true_valid_but_linear_invalid, both_invalid, true_invalid_but_linear_valid"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T03:44:02.755220Z",
     "start_time": "2023-12-14T03:44:02.752133Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:00<00:00, 35069.87it/s]\n",
      "100%|██████████| 200000/200000 [00:05<00:00, 34615.10it/s]\n",
      "100%|██████████| 20000/20000 [00:00<00:00, 402988.47it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 566816.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passes both heuristics: 19116\n",
      "Fails both heuristics: 198239\n",
      "Passes hierarch, fails linear: 884\n",
      "Fails hierarch, passes linear: 1761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "Stringifying the column:   0%|          | 0/3200 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a268c7bc611e4a6eb587e7aedb78f14f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Casting to class labels:   0%|          | 0/3200 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cf704def2e1c42bb9f7b499acfeff4f5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Stringifying the column:   0%|          | 0/800 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "39acedb053b34b11bb48b791d7a1f8c7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Casting to class labels:   0%|          | 0/800 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2c157f91a765482a9945bd0675a17dde"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "14a91d420eeb4d2887fb01ad5515346b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Creating parquet from Arrow format:   0%|          | 0/3 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "19c151cd510f4d41bb6645675cc33729"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b3b4bc8ad9da44cda6378771df961af4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "383664f91b2f4ef19af0340420ce447f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "032d8e77d86a401dac65ba402c35ea0b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1e0c322070404911888ce3acd472481c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "README.md:   0%|          | 0.00/747 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "51865d599fda437f98df661a5a073556"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create generalization scenario\n",
    "train_examples = 3200\n",
    "\n",
    "valid_num = 20000\n",
    "invalid_num = 200000\n",
    "\n",
    "both_valid, true_valid_but_linear_invalid, both_invalid, true_invalid_but_linear_valid = generate_with_heuristic(pcfg, valid_num, invalid_num)\n",
    "\n",
    "train_sents = [sent for sent, _ in both_valid[:1600] + both_invalid[:1600]]\n",
    "train_depths = [depth for _, depth in both_valid[:1600] + both_invalid[:1600]]\n",
    "train_labels = [1] * 1600 + [0] * 1600\n",
    "\n",
    "eval_sents = [sent for sent, _ in true_valid_but_linear_invalid[:400] + true_invalid_but_linear_valid[:400]]\n",
    "eval_depths = [depth for _, depth in true_valid_but_linear_invalid[:400] + true_invalid_but_linear_valid[:400]]\n",
    "eval_labels = [1] * 400 + [0] * 400\n",
    "\n",
    "dataset_gen_train = datasets.Dataset.from_dict({\"text\": train_sents, \"labels\": train_labels, \"depth\": train_depths}).shuffle()\n",
    "dataset_gen_eval = datasets.Dataset.from_dict({\"text\": eval_sents, \"labels\": eval_labels, \"depth\": eval_depths}).shuffle()\n",
    "\n",
    "dataset_gen_train_eval = dataset_gen_train.class_encode_column('labels').train_test_split(test_size=0.25, stratify_by_column='labels')\n",
    "dataset_gen_test = dataset_gen_eval.class_encode_column('labels')\n",
    "\n",
    "gen_dataset_dict = datasets.DatasetDict({\n",
    "    'train': dataset_gen_train_eval['train'],\n",
    "    'eval': dataset_gen_train_eval['test'],\n",
    "    'test': dataset_gen_test\n",
    "})\n",
    "# dataset.train_test_split(test_size=0.3, stratify_by_column='labels')\n",
    "gen_dataset_dict.push_to_hub(\"michaelginn/latent-trees-agreement-GEN\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T03:45:27.805647Z",
     "start_time": "2023-12-14T03:44:03.943631Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['text', 'labels'],\n        num_rows: 2400\n    })\n    eval: Dataset({\n        features: ['text', 'labels'],\n        num_rows: 800\n    })\n    test: Dataset({\n        features: ['text', 'labels'],\n        num_rows: 800\n    })\n})"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_dataset_dict"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T02:34:32.717284Z",
     "start_time": "2023-12-14T02:34:32.711043Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Extreme Generalization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 1949.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a cat that kicks the wugs that fight these ladies that fight those rabbits that kiss a dude and these boys that kick these physicists and those birds that kiss the birds that love the bird that hugs the dog that hugs a rabbit that loves those ducks that love those dudes and those wugs that fight the lady that kicks these linguists that kiss the boy that kisses those physicists that hug a boy that hugs these ducks and those girls and the birds that fight a girl that hugs a shiny boy and the boys that kick me and those physicists that hug a boy that punches the cats and those ladies that walk kisses those birds that think', 10), ('those boys that walk and kiss you and ponder and love these physicists that kiss the linguists that love the wugs that hug the dude that kisses the boy and laugh and ponder and the rabbits that walk punch a duck that hugs these cats that love those cheeses and these girls that kiss these blue physicists that punch a cat that hugs those girls that love the boy that ponders and hugs you and the cats that fight them and those dudes that fight the dudes and fight the turtle that laughs and these cats that kiss the cat that fights a cheese that loves those girls that kiss the turtle that fights a turtle that hugs those sparkling linguists and punches the physicist that punches a physicist and kiss the rabbit and these big linguists that punch these linguists', 8), ('the boys that punch the cat that kisses the dude that kicks the girls that love those rutabagas that love the dog that kisses the rabbit that punches a rutabaga that loves a dude that kicks the ladies and loves these dogs that kick those ladies that ponder and fight a mad boy that kicks the cats that hug us and the red rutabagas and the cats that punch those linguists that punch the linguist that kisses those ladies that hug those boys that kiss these linguists that punch these ladies that punch those rutabagas that kiss the linguists that hug these dudes that kick those turtles that fight these rutabagas that kick the cats that kiss them and the red boys that punch the rutabaga that loves these birds that fight the linguist that loves the cat that fights these boys that punch these wugs that love a rabbit that kicks the dog that hugs these cheeses that kick us and loves these linguists that punch the rabbit and the rabbits that kick me and these happy rabbits and laugh and kiss a dog that kicks those big ducks that punch me and loves the physicist that runs and thinks and those dogs that kick the girls that fight the cat that hugs those physicists that fight these rutabagas that kick a lady and these boys that love the turtles that kick a turtle and love those girls that hug the red linguist that loves a rabbit that loves those shiny linguists that kiss the turtle and these ladies that punch a girl that fights these rutabagas that kiss those boys that run and these dogs that punch those ducks that kick the wugs that fight these girls that ponder and kick those rutabagas that fight a boy that fights those birds that hug the physicist that kisses a duck and those rabbits and the cats that kiss the cat that loves the rabbits that love the girls that love the boys that punch these cats that punch these birds that think and fight a linguist that kisses the rutabaga and those ladies that fight the physicist that punches those wugs that fight the dude that hugs a girl that loves these cats that hug the girl that laughs and those birds that love the ladies that kiss the bird and these cheeses that love the wugs that hug those dudes that kick the turtle that kisses the ladies that punch the boy that hugs the girl that punches these cheeses that fight the cats that fight the cheeses that laugh and fight the physicist and these cheeses that kiss these physicists and those dudes that punch the rutabaga that punches the linguist that kisses those cats and fight the physicist that punches a duck and think and the cheeses that run and the rutabagas that hug a cat that walks and those cheeses hug these rabbits that kick these boys that kiss the linguists that kick a wug that kicks the turtle that walks', 28), ('those turtles that love those ladies that walk love the dog that kisses a rabbit that loves those turtles', 2), ('those birds that love the bird kiss the physicists that think', 1), ('these rutabagas that love these ladies that kiss these rutabagas that kiss those mad turtles and these ducks that kick a boy that kisses a dude that runs and these girls that kiss these cheeses that ponder love a boy that punches these ducks', 5), ('a linguist that kicks the rutabagas that fight the boys that love the lady that kisses those ladies that fight the linguists that kiss the physicist and these boys and hug the dog that laughs and kick a rutabaga that kisses those girls that love the boy that kicks a rutabaga and those physicists that hug the rutabagas that punch the ladies that hug a linguist that loves the rabbits that hug the linguists that kick those turtles that fight you and the wugs that kick the cat that fights the shiny physicist that kicks those cheeses that fight the ladies that kiss us and these girls that hug the physicist that punches these ladies that ponder and those linguists that hug these rabbits and punch a physicist that laughs and kisses the physicist that ponders and kicks a cat that ponders and kisses the cheese that kisses the wug and fight a duck that kisses them kicks the duck that loves a wug that kicks the duck that loves the boy that kicks a turtle that kisses us', 17), ('a linguist that loves a turtle that fights me loves the duck that kisses a rabbit that hugs a turtle that loves a lady that punches a cat that loves those boys that punch these turtles', 6), ('a cat fights the dudes that fight the girl that loves the rabbits that laugh and the linguists that hug a cat that punches a turtle that walks and love those cheeses that punch those physicists that hug you and kiss a wug that walks and those turtles that love a linguist', 6), ('those turtles that punch the cat that kisses us fight a dog that hugs the girls that kiss the rabbit that punches the turtles that kiss the linguist that hugs these linguists that kick the girl that kisses the physicist that fights a dude that kisses the bird that ponders and these wugs that punch a girl that kisses these cats that kick those birds that fight these cats that punch the boy that kicks the girl that kicks a bird that ponders and those ducks that kick the physicists that kick us and the cats that love the rutabagas that love a happy cat and those cats that fight me', 13), ('we hug the girls that hug those turtles that kiss the cat that hugs a duck that loves a rabbit that runs and these dogs that kiss the cat that thinks and those turtles that fight me and the wugs that punch a cat that hugs those rabbits that hug a boy that loves those wugs and loves the girl that kicks a duck that loves these rabbits that think and kisses the girls that ponder and those birds that punch those physicists that punch the lady that walks and the boys that punch me', 8), ('the rabbits that hug these linguists that punch a lady that loves the dog that fights the rabbit that kisses the boy that thinks and punches the duck that kicks these cats that hug those dogs that love a rabbit that loves the ducks and these girls that kiss the cat that kicks the cat that punches the duck that punches me and the cheeses that fight the physicist that punches those birds that kick the boy that loves us and these girls and these dudes that kiss the duck that punches those cats that love the boys that love a cat that hugs the mad turtles that punch the cheese that hugs the linguist that punches a rabbit and those girls and these physicists that fight a lady that kisses those birds that hug the ducks that fight the turtles and punches a rutabaga kick a girl that hugs you', 10), ('these linguists that fight the physicist that loves a girl that walks fight these ladies that kick us', 3), ('the cheese that kicks those dudes that punch the cats that fight a cat that kicks these ducks loves the cat that laughs', 4), ('these cats that kick a physicist that kisses a girl that ponders and the birds kick those girls', 3), ('the turtles that run love those boys that hug those dogs', 1), ('these happy linguists love a cheese that kicks these sparkling girls that fight us', 2), ('the wug that kisses the cat that kisses the ladies that fight these cheeses that kiss them and hug a cheese that thinks and kisses the physicist kicks the linguists that ponder', 4), ('a girl that fights the bird kisses the birds that hug a rabbit that runs and fights the cat that punches these ladies that kiss a girl that kicks those linguists and the girls that laugh and kisses a duck that loves them', 6), ('these turtles that hug a rabbit that loves a lady that fights the linguists punch those dogs that hug the turtles that kiss the physicists that love a boy that hugs these wugs that kiss these rabbits that kiss the linguists that run and kiss a dude that kicks the cheese that kisses the lady that kisses those physicists and loves a turtle that loves these cheeses and kicks the dude that laughs and kicks those dudes that kiss a rutabaga that thinks and the rutabagas that kick those sparkling ladies that fight these rabbits that fight the rabbits that fight a bird that punches the turtle that kisses a wug that fights the duck that fights these cats and these ladies that run and hugs the cat and those birds that kick those dogs that kiss a turtle that hugs the red sparkling cheeses and loves those linguists that fight these cats that punch the rutabaga that hugs the cat that walks and the rabbits and those physicists that think and these birds that kiss these birds that fight those ladies that kiss us and these boys that hug the cats that kick you and kiss the wugs that kiss them and those dudes that hug those physicists that kiss a linguist that hugs them and hug the girl that loves a lady and fights the physicists that ponder and fights me and kiss a boy that punches the bird that loves a turtle and these dudes and the birds', 15)]\n",
      "Average depth 7.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "extreme_pcfg = PCFG.fromstring(f\"\"\"\n",
    "S             -> NP_3Sg_nom VP_3Sg [0.5] | NP_nom VP [0.5]\n",
    "\n",
    "VP_3Sg        -> VT '+s' NP_acc [0.8] | VI '+s' [0.1] | VP_3Sg 'and' VP_3Sg [0.1]\n",
    "VP            -> VT      NP_acc [0.8] | VI      [0.1] | VP     'and' VP     [0.1]\n",
    "\n",
    "NP_3Sg_nom    -> 'he' [0.05] | 'she' [0.05] | NP_common_Sg [0.9]\n",
    "NP_common_Sg  -> Det_Sg N_bar_common_Sg [1]\n",
    "Det_Sg        -> {equal_production('the | a')}\n",
    "\n",
    "NP_nom        -> {equal_production('I | you | we | they', total=0.1)} | NP_common_Pl [0.9]\n",
    "NP_common_Pl  -> Det_Pl N_bar_common_Pl [0.8] | NP_common_Pl 'and' NP_common_Pl [0.2]\n",
    "Det_Pl        -> {equal_production('the | those | these')}\n",
    "\n",
    "NP_acc        -> {equal_production('me | you | us | them', total=0.1)} | NP_common_Pl [0.45] | NP_common_Sg [0.45]\n",
    "\n",
    "N_bar_common_Sg  -> Adj N_bar_common_Sg [0.05] | N_common Rel_Sg [0.8] | N_common [0.15]\n",
    "N_bar_common_Pl  -> Adj N_bar_common_Pl [0.05] | N_common '+s' Rel_Pl [0.8] | N_common '+s' [0.15]\n",
    "N_common      -> {equal_production('girl | boy | cat | turtle | rutabaga | duck | cheese | dude | rabbit | wug | linguist | physicist | lady | dog | cat | bird')}\n",
    "\n",
    "Rel_Sg         -> 'that' VP_3Sg [1]\n",
    "Rel_Pl         -> 'that' VP [1]\n",
    "\n",
    "VI            -> {equal_production('run | walk | think | laugh | ponder')}\n",
    "VT            -> {equal_production('kick | kiss | hug | punch | fight | love')}\n",
    "\n",
    "Adj           -> {equal_production('big | small | happy | mad | red | blue | sparkling | shiny')}\n",
    "\"\"\")\n",
    "\n",
    "sents = sample_sentences(extreme_pcfg, 20)\n",
    "\n",
    "print(sents)\n",
    "print(\"Average depth\", sum(depth for _, depth in sents) / len(sents))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T03:52:09.539987Z",
     "start_time": "2023-12-14T03:52:09.392058Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "data": {
      "text/plain": "Stringifying the column:   0%|          | 0/800 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "750e961916d3424eb09dde5d6e8dd634"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Casting to class labels:   0%|          | 0/800 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "48452ad0212346a79023037a42f3f1b0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e448a80ce8344c9c8299c9201492b6b6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Creating parquet from Arrow format:   0%|          | 0/3 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "64551b0aedb74836b0b9fded16a3ed7a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5418614f5b7d47518babe1fb177cd9e3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fbb2160bd14e4ebcaa9110d5a10a1a10"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9dd8fda1fdb54b9e8335998ec64a31d0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f2596fa846204064b5a39504a416bd04"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "README.md:   0%|          | 0.00/603 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "beb8224bff9c47c7bcfc775c8f57cacb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create extreme generalization scenario\n",
    "train_examples = 3200\n",
    "\n",
    "# both_valid, true_valid_but_linear_invalid, both_invalid, true_invalid_but_linear_valid = generate_with_heuristic(extreme_pcfg, 2000, 300000)\n",
    "\n",
    "# dataset_extreme_train = datasets.Dataset.from_dict({\"text\": both_valid[:1600] + both_invalid[:1600], \"labels\": [1] * 1600 + [0] * 1600}).shuffle()\n",
    "\n",
    "true_valid_but_linear_invalid = [(sent, depth) for sent, depth in true_valid_but_linear_invalid if depth > 3]\n",
    "true_invalid_but_linear_valid = [(sent, depth) for sent, depth in true_invalid_but_linear_valid if depth > 3]\n",
    "\n",
    "# Sort by depth\n",
    "true_valid_but_linear_invalid.sort(key=lambda x: x[1])\n",
    "true_invalid_but_linear_valid.sort(key=lambda x: x[1])\n",
    "\n",
    "extreme_eval_sents = [sent for sent, _ in true_valid_but_linear_invalid[:400] + true_invalid_but_linear_valid[:400]]\n",
    "extreme_eval_depths = [depth for _, depth in true_valid_but_linear_invalid[:400] + true_invalid_but_linear_valid[:400]]\n",
    "extreme_eval_labels = [1] * 400 + [0] * 400\n",
    "\n",
    "dataset_extreme_eval = datasets.Dataset.from_dict({\"text\": extreme_eval_sents, \"labels\": extreme_eval_labels, \"depth\": extreme_eval_depths}).shuffle()\n",
    "\n",
    "# dataset_extreme_train_eval = dataset_extreme_train.class_encode_column('labels').train_test_split(test_size=0.25, stratify_by_column='labels')\n",
    "dataset_extreme_eval = dataset_extreme_eval.class_encode_column('labels')\n",
    "\n",
    "extreme_dataset_dict = datasets.DatasetDict({\n",
    "    'train': dataset_gen_train_eval['train'],\n",
    "    'eval': dataset_gen_train_eval['test'],\n",
    "    'test': dataset_extreme_eval\n",
    "})\n",
    "extreme_dataset_dict.push_to_hub(\"michaelginn/latent-trees-agreement-GENX\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T05:57:04.185967Z",
     "start_time": "2023-12-14T05:56:59.737117Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/800 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "40e23b6581e541c7ba9f335e7ddd68d2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "252\n",
      "52.41625\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# max_length = 100\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example['text'])\n",
    "dataset_extreme_eval_tok = dataset_extreme_eval.map(tokenize_function, batched=True, load_from_cache_file=False)\n",
    "\n",
    "# Get longest token sequence length\n",
    "max_length = max(len(x) for x in dataset_extreme_eval_tok[\"input_ids\"])\n",
    "print(max_length)\n",
    "\n",
    "# Average length\n",
    "print(sum(len(x) for x in dataset_extreme_eval_tok[\"input_ids\"]) / len(dataset_extreme_eval_tok[\"input_ids\"]))\n",
    "\n",
    "# Number of rows with more than 512 tokens\n",
    "print(sum(len(x) > 512 for x in dataset_extreme_eval_tok[\"input_ids\"]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T05:57:08.359374Z",
     "start_time": "2023-12-14T05:57:07.576760Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
